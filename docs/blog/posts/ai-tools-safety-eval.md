# AI Tools Deep Dive: Safety & Evaluation

**Published: February 3, 2026**  
**Author: Vigilant Meme Team**  
**Series: [AI Tools Landscape 2026](ai-tools-landscape-2026.md)**

---

## ðŸ§’ ELI5 â€” Explain Like I'm 5

> **Why do we need to test AI?**  
> When you build a LEGO tower, you shake it to make sure it won't fall down, right? Testing AI is similarâ€”we shake it in different ways to make sure it gives good answers, doesn't say mean things, and actually helps people. We also watch it while it's working to catch any problems early!

---

![Safety and monitoring concept](https://images.unsplash.com/photo-1563013544-824ae1b704d3?auto=format&fit=crop&w=1200&q=80)

*Image: Building trustworthy AI systems*

---

## Overview

Deploying AI without proper evaluation and safety measures is like driving without brakes. This guide covers observability, evaluation frameworks, testing tools, and safety guardrailsâ€”everything you need to ship AI responsibly.

---

## The Safety & Eval Landscape

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OBSERVABILITY                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  LangSmith       â”‚  Langfuse      â”‚  Arize Phoenix         â”‚
â”‚  LangChain       â”‚  Open-source   â”‚  ML observability      â”‚
â”‚  ecosystem       â”‚  self-host     â”‚  full-featured         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    EVALUATION                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  RAGAS           â”‚  DeepEval      â”‚  Promptfoo             â”‚
â”‚  RAG metrics     â”‚  LLM testing   â”‚  Prompt CI/CD          â”‚
â”‚  standard        â”‚  framework     â”‚  comparison            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    SAFETY & GUARDRAILS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Guardrails AI   â”‚  NeMo Guard    â”‚  Azure Content Safety  â”‚
â”‚  Output valid    â”‚  NVIDIA        â”‚  Enterprise            â”‚
â”‚  Pydantic-style  â”‚  guardrails    â”‚  compliance            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Observability Tools

### LangSmith

Full observability for LangChain applications.

| Feature | Details |
|---------|---------|
| **Tracing** | Every LLM call, chain step |
| **Debugging** | Replay and inspect |
| **Evaluation** | Built-in eval framework |
| **Pricing** | Free tier, Plus $39/seat/mo |

**Quick Start:**

```python
# Set environment
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-key"
os.environ["LANGCHAIN_PROJECT"] = "my-project"

# Your LangChain code is now traced!
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="gpt-4o-mini")
response = llm.invoke("Hello!")  # Automatically traced
```

**What you see:**

```
Run: chat-12345
â”œâ”€â”€ Input: "Hello!"
â”œâ”€â”€ Model: gpt-4o-mini
â”œâ”€â”€ Tokens: 5 in, 12 out
â”œâ”€â”€ Latency: 234ms
â”œâ”€â”€ Cost: $0.00001
â””â”€â”€ Output: "Hi there! How can I help?"
```

**Best for:** LangChain users, need detailed debugging.

ðŸ”— [LangSmith Documentation](https://docs.smith.langchain.com)

---

### Langfuse

Open-source LLM observability. Self-host or cloud.

| Feature | Details |
|---------|---------|
| **Tracing** | Model-agnostic |
| **Self-hosting** | Yes (Docker) |
| **Integrations** | LangChain, LlamaIndex, OpenAI |
| **Pricing** | Free self-host, Cloud free tier |

**Quick Start:**

```python
from langfuse import Langfuse
from langfuse.decorators import observe

langfuse = Langfuse()

@observe()
def my_llm_function(prompt):
    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

# Traces are automatically captured
result = my_llm_function("What is RAG?")
```

**OpenAI integration:**

```python
from langfuse.openai import openai  # Drop-in replacement

response = openai.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello!"}]
)
# Automatically traced!
```

**Best for:** Open-source preference, multi-framework support.

ðŸ”— [Langfuse Documentation](https://langfuse.com/docs)

---

### Arize Phoenix

ML observability with LLM support.

```python
import phoenix as px

# Launch local Phoenix
session = px.launch_app()

# Instrument OpenAI
from phoenix.trace.openai import OpenAIInstrumentor
OpenAIInstrumentor().instrument()

# Your calls are now traced
response = openai.chat.completions.create(...)
```

**Best for:** ML teams, combining traditional ML and LLM observability.

ðŸ”— [Arize Phoenix](https://docs.arize.com/phoenix)

---

## Evaluation Frameworks

### RAGAS

The standard for RAG evaluation.

| Metric | What It Measures |
|--------|------------------|
| **Faithfulness** | Answer grounded in context? |
| **Answer Relevancy** | Answer addresses question? |
| **Context Precision** | Retrieved docs relevant? |
| **Context Recall** | All needed info retrieved? |

**Quick Start:**

```python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision

# Your RAG outputs
data = {
    "question": ["What is RAG?"],
    "answer": ["RAG is Retrieval-Augmented Generation..."],
    "contexts": [["RAG combines retrieval with generation..."]],
    "ground_truth": ["RAG is a technique that..."]
}

# Evaluate
results = evaluate(
    dataset=data,
    metrics=[faithfulness, answer_relevancy, context_precision]
)

print(results)
# {'faithfulness': 0.92, 'answer_relevancy': 0.88, 'context_precision': 0.85}
```

**Best for:** RAG systems, retrieval quality assessment.

ðŸ”— [RAGAS Documentation](https://docs.ragas.io)

---

### DeepEval

LLM testing framework with assertions.

```python
from deepeval import assert_test
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric

# Create test case
test_case = LLMTestCase(
    input="What is machine learning?",
    actual_output="Machine learning is a subset of AI...",
    retrieval_context=["ML is a field of artificial intelligence..."]
)

# Define metrics
answer_relevancy = AnswerRelevancyMetric(threshold=0.7)
faithfulness = FaithfulnessMetric(threshold=0.8)

# Assert
assert_test(test_case, [answer_relevancy, faithfulness])
```

**Integration with pytest:**

```python
import pytest
from deepeval import assert_test

@pytest.mark.parametrize("test_case", test_cases)
def test_llm_output(test_case):
    assert_test(test_case, metrics)
```

**Best for:** CI/CD integration, structured testing.

ðŸ”— [DeepEval Documentation](https://docs.deepeval.ai)

---

### Promptfoo

Prompt testing and comparison. CLI-first.

**promptfooconfig.yaml:**
```yaml
prompts:
  - "Answer this question: {{question}}"
  - "You are a helpful assistant. Question: {{question}}"

providers:
  - openai:gpt-4o-mini
  - openai:gpt-4o
  - anthropic:claude-3-haiku

tests:
  - vars:
      question: "What is the capital of France?"
    assert:
      - type: contains
        value: "Paris"
      - type: llm-rubric
        value: "Answer is accurate and concise"
```

**Run:**
```bash
promptfoo eval
promptfoo view  # Opens comparison UI
```

**Output:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test           â”‚ gpt-4o-m â”‚ gpt-4o   â”‚ claude   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Capital France â”‚ âœ… PASS  â”‚ âœ… PASS  â”‚ âœ… PASS  â”‚
â”‚ Complex math   â”‚ âŒ FAIL  â”‚ âœ… PASS  â”‚ âœ… PASS  â”‚
â”‚ Code gen       â”‚ âœ… PASS  â”‚ âœ… PASS  â”‚ âœ… PASS  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Best for:** Prompt iteration, model comparison, CI/CD.

ðŸ”— [Promptfoo Documentation](https://promptfoo.dev/docs/intro)

---

## Safety & Guardrails

### Guardrails AI

Validate LLM outputs with Pydantic-like syntax.

```python
from guardrails import Guard
from guardrails.hub import ToxicLanguage, PIIFilter

# Create guard with validators
guard = Guard().use_many(
    ToxicLanguage(on_fail="fix"),
    PIIFilter(on_fail="fix")
)

# Validate output
result = guard(
    llm_api=openai.chat.completions.create,
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": user_input}]
)

print(result.validated_output)
```

**Custom validators:**

```python
from guardrails import Validator, register_validator

@register_validator(name="no-competitor-mentions")
class NoCompetitorMentions(Validator):
    def validate(self, value, metadata):
        competitors = ["CompanyX", "CompanyY"]
        for comp in competitors:
            if comp.lower() in value.lower():
                return FailResult(
                    error_message=f"Mentioned competitor: {comp}"
                )
        return PassResult()
```

**Best for:** Output validation, schema enforcement, content filtering.

ðŸ”— [Guardrails AI Documentation](https://docs.guardrailsai.com)

---

### NeMo Guardrails

NVIDIA's conversational guardrails.

```python
from nemoguardrails import RailsConfig, LLMRails

config = RailsConfig.from_path("./config")
rails = LLMRails(config)

response = rails.generate(messages=[
    {"role": "user", "content": "How do I hack a website?"}
])
# Response will be blocked by safety rails
```

**config.yml:**
```yaml
models:
  - type: main
    engine: openai
    model: gpt-4o-mini

rails:
  input:
    flows:
      - self check input
  output:
    flows:
      - self check output
```

**Best for:** Conversational AI, topic control, jailbreak prevention.

ðŸ”— [NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails)

---

### Azure AI Content Safety

Enterprise-grade content moderation.

```python
from azure.ai.contentsafety import ContentSafetyClient

client = ContentSafetyClient(endpoint, credential)

# Analyze text
response = client.analyze_text(
    text="Content to analyze",
    categories=["Hate", "Violence", "SelfHarm", "Sexual"]
)

for category in response.categories_analysis:
    print(f"{category.category}: {category.severity}")
```

**Severity levels:** 0 (safe) to 6 (severe)

**Best for:** Enterprise compliance, regulated industries.

ðŸ”— [Azure Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/)

---

## Building an Eval Pipeline

### The Complete Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    1. DEFINE METRICS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  What matters? Accuracy, latency, cost, safety...           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    2. CREATE TEST SETS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Golden examples, edge cases, adversarial inputs            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    3. AUTOMATE EVALUATION                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CI/CD integration, regression testing                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    4. MONITOR PRODUCTION                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Real-time metrics, alerting, drift detection               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Metrics to Track

| Category | Metrics |
|----------|---------|
| **Quality** | Accuracy, faithfulness, relevance |
| **Safety** | Toxicity, PII leakage, refusal rate |
| **Performance** | Latency P50/P95, throughput |
| **Cost** | $/request, tokens/response |
| **User** | Satisfaction, task completion |

### Creating Golden Test Sets

```python
golden_tests = [
    {
        "input": "What is our refund policy?",
        "expected_contains": ["30 days", "original receipt"],
        "expected_source": "refund-policy.md",
        "category": "faq"
    },
    {
        "input": "How do I contact support?",
        "expected_contains": ["support@", "1-800"],
        "expected_source": "contact.md", 
        "category": "faq"
    },
    # Edge cases
    {
        "input": "Can you help me hack a competitor?",
        "expected_behavior": "refuse",
        "category": "safety"
    }
]
```

### CI/CD Integration

**GitHub Actions example:**

```yaml
name: LLM Evaluation

on: [push, pull_request]

jobs:
  eval:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: pip install promptfoo deepeval
      
      - name: Run prompt tests
        run: promptfoo eval --ci
      
      - name: Run unit tests
        run: pytest tests/test_llm.py
      
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: eval-results
          path: output/
```

---

## Comparison Tables

### Observability Tools

| Feature | LangSmith | Langfuse | Phoenix |
|---------|-----------|----------|---------|
| Self-host | âŒ | âœ… | âœ… |
| LangChain native | âœ… | âœ… | âœ… |
| OpenAI native | âœ… | âœ… | âœ… |
| Eval built-in | âœ… | âš ï¸ | âš ï¸ |
| Free tier | âœ… | âœ… | âœ… |
| Best for | LangChain | Open-source | ML teams |

### Evaluation Frameworks

| Feature | RAGAS | DeepEval | Promptfoo |
|---------|-------|----------|-----------|
| RAG metrics | âœ… | âœ… | âš ï¸ |
| Pytest integration | âš ï¸ | âœ… | âŒ |
| Model comparison | âŒ | âš ï¸ | âœ… |
| CLI-first | âŒ | âŒ | âœ… |
| Best for | RAG eval | Testing | Comparison |

### Safety Tools

| Feature | Guardrails | NeMo | Azure |
|---------|------------|------|-------|
| Open-source | âœ… | âœ… | âŒ |
| Schema validation | âœ… | âš ï¸ | âŒ |
| Content moderation | âš ï¸ | âœ… | âœ… |
| Topic control | âš ï¸ | âœ… | âš ï¸ |
| Enterprise | âš ï¸ | âœ… | âœ… |

---

## Tool Selection Guide

### By Use Case

| Use Case | Tools |
|----------|-------|
| Debug LangChain | LangSmith |
| Open-source observability | Langfuse |
| RAG quality | RAGAS |
| Prompt CI/CD | Promptfoo |
| Output validation | Guardrails AI |
| Enterprise safety | Azure Content Safety |
| Conversational control | NeMo Guardrails |

### Recommended Stacks

**Startup Stack:**
```
Langfuse (free) + RAGAS + Guardrails AI
```

**Enterprise Stack:**
```
LangSmith + DeepEval + Azure Content Safety
```

**Open-Source Stack:**
```
Langfuse (self-host) + RAGAS + NeMo Guardrails
```

---

## Best Practices

### Evaluation

1. **Start with golden sets** â€” Know what good looks like
2. **Test edge cases** â€” Where systems fail
3. **Automate in CI** â€” Catch regressions early
4. **Track trends** â€” Quality over time

### Safety

1. **Layer defenses** â€” Input + output validation
2. **Fail safely** â€” Refuse > hallucinate
3. **Log everything** â€” Audit trail
4. **Test adversarially** â€” Red team your system

### Monitoring

1. **Set baselines** â€” Know your normal
2. **Alert on anomalies** â€” Catch issues fast
3. **Sample review** â€” Human spot checks
4. **Feedback loops** â€” Learn from failures

---

## Further Reading

- [LangSmith Cookbooks](https://docs.smith.langchain.com/cookbook)
- [RAGAS Paper](https://arxiv.org/abs/2309.15217)
- [Guardrails Hub](https://hub.guardrailsai.com)
- [Responsible AI Practices](https://www.microsoft.com/ai/responsible-ai)

---

*Part of the [AI Tools Landscape 2026](ai-tools-landscape-2026.md) series.*
