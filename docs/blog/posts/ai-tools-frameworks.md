# AI Tools Deep Dive: Frameworks & Orchestration

**Published: February 3, 2026**  
**Author: Vigilant Meme Team**  
**Series: [AI Tools Landscape 2026](ai-tools-landscape-2026.md)**

---

## üßí ELI5 ‚Äî Explain Like I'm 5

> **What are AI frameworks?**  
> Imagine you're building with LEGO. You could connect each brick yourself, or you could use special connector pieces that make it easier to build bigger things faster. AI frameworks are like those special connectors‚Äîthey help you plug together AI models, databases, and tools without writing everything from scratch!

---

![Framework and orchestration concept](https://images.unsplash.com/photo-1558494949-ef010cbdcc31?auto=format&fit=crop&w=1200&q=80)

*Image: Connecting AI components into unified systems*

---

## Overview

AI frameworks handle the "plumbing" between models, data sources, and application logic. They provide abstractions for common patterns like RAG, agents, and chains, letting you focus on your application instead of infrastructure.

---

## Framework Landscape

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    GENERAL PURPOSE                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  LangChain       ‚îÇ  Semantic Kernel  ‚îÇ  Haystack            ‚îÇ
‚îÇ  Python/JS       ‚îÇ  C#/Python/Java   ‚îÇ  Python              ‚îÇ
‚îÇ  Broadest        ‚îÇ  Enterprise .NET  ‚îÇ  Production          ‚îÇ
‚îÇ  ecosystem       ‚îÇ  focus            ‚îÇ  search focus        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                    RAG FOCUSED                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  LlamaIndex      ‚îÇ  Embedchain       ‚îÇ  RAGFlow             ‚îÇ
‚îÇ  Best for RAG    ‚îÇ  Simple RAG       ‚îÇ  Visual RAG          ‚îÇ
‚îÇ  pipelines       ‚îÇ  quick start      ‚îÇ  builder             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                    AGENT FOCUSED                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  LangGraph       ‚îÇ  AutoGen          ‚îÇ  CrewAI              ‚îÇ
‚îÇ  Stateful        ‚îÇ  Multi-agent      ‚îÇ  Role-based          ‚îÇ
‚îÇ  workflows       ‚îÇ  conversations    ‚îÇ  agents              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                    LIGHTWEIGHT / DIRECT                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Instructor      ‚îÇ  Marvin           ‚îÇ  LMQL                ‚îÇ
‚îÇ  Structured      ‚îÇ  AI functions     ‚îÇ  Query               ‚îÇ
‚îÇ  outputs         ‚îÇ  for Python       ‚îÇ  language            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## General Purpose Frameworks

### LangChain

The most popular AI framework. Huge ecosystem, rapid development.

| Aspect | Details |
|--------|---------|
| **Language** | Python, JavaScript/TypeScript |
| **Maturity** | Production-ready |
| **Ecosystem** | Largest (integrations, tutorials, community) |
| **Learning curve** | Medium |
| **Best for** | Rapid prototyping, broad integrations |

**Core Concepts:**

```python
# 1. Chat Models - Talk to LLMs
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="gpt-4o-mini")

# 2. Prompts - Template your inputs
from langchain_core.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("user", "{input}")
])

# 3. Chains - Connect components
chain = prompt | llm

# 4. Output Parsers - Structure responses
from langchain_core.output_parsers import StrOutputParser
chain = prompt | llm | StrOutputParser()

# 5. Run it
response = chain.invoke({"input": "Hello!"})
```

**Key Components:**

| Component | Purpose |
|-----------|---------|
| **LCEL** | LangChain Expression Language for composing chains |
| **Retrievers** | Get relevant documents for RAG |
| **Tools** | Give LLMs abilities (search, calculate, etc.) |
| **Memory** | Maintain conversation history |
| **Callbacks** | Logging, tracing, streaming |

**Strengths:**
- ‚úÖ Massive ecosystem (100+ integrations)
- ‚úÖ Great documentation and tutorials
- ‚úÖ Active community
- ‚úÖ LangSmith for observability
- ‚úÖ Rapid iteration

**Weaknesses:**
- ‚ùå Can be over-abstracted
- ‚ùå Breaking changes between versions
- ‚ùå Sometimes "magic" is hard to debug
- ‚ùå Overhead for simple use cases

**When to use:** Most projects, especially if you need many integrations or rapid prototyping.

üîó [LangChain Documentation](https://python.langchain.com/docs)

---

### LangGraph

Stateful, graph-based workflows. Part of LangChain ecosystem.

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict

# Define state
class AgentState(TypedDict):
    messages: list
    next_action: str

# Define nodes (functions)
def call_model(state):
    response = llm.invoke(state["messages"])
    return {"messages": state["messages"] + [response]}

def should_continue(state):
    if needs_tool(state):
        return "tool"
    return "end"

# Build graph
workflow = StateGraph(AgentState)
workflow.add_node("agent", call_model)
workflow.add_node("tool", call_tool)
workflow.add_conditional_edges("agent", should_continue)
workflow.add_edge("tool", "agent")

# Compile and run
app = workflow.compile()
result = app.invoke({"messages": [user_message]})
```

**Best for:** Complex agent workflows, stateful applications, human-in-the-loop.

üîó [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)

---

### Semantic Kernel

Microsoft's framework for .NET, Python, and Java.

| Aspect | Details |
|--------|---------|
| **Language** | C#, Python, Java |
| **Maturity** | Production-ready |
| **Ecosystem** | Microsoft/Azure focused |
| **Learning curve** | Medium |
| **Best for** | Enterprise .NET applications |

**Core Concepts:**

```csharp
// C# Example
using Microsoft.SemanticKernel;

// Create kernel
var kernel = Kernel.CreateBuilder()
    .AddAzureOpenAIChatCompletion(
        deploymentName: "gpt-4",
        endpoint: "https://your-resource.openai.azure.com",
        apiKey: "your-key"
    )
    .Build();

// Define a plugin (function)
public class TimePlugin
{
    [KernelFunction]
    public string GetCurrentTime() => DateTime.Now.ToString();
}

kernel.Plugins.AddFromType<TimePlugin>();

// Use it
var result = await kernel.InvokePromptAsync(
    "What time is it? Use the time plugin."
);
```

**Key Concepts:**

| Concept | Description |
|---------|-------------|
| **Kernel** | Central orchestrator |
| **Plugins** | Collections of functions |
| **Functions** | Semantic (prompts) or Native (code) |
| **Planners** | Automatic function orchestration |
| **Memory** | Vector store integration |

**Strengths:**
- ‚úÖ First-class .NET support
- ‚úÖ Native Azure integration
- ‚úÖ Enterprise-grade design
- ‚úÖ Strong typing
- ‚úÖ Microsoft backing

**Weaknesses:**
- ‚ùå Smaller community than LangChain
- ‚ùå Fewer third-party integrations
- ‚ùå Documentation can lag features

**When to use:** .NET applications, Azure-heavy environments, enterprise projects.

üîó [Semantic Kernel Documentation](https://learn.microsoft.com/semantic-kernel/)

---

### Haystack

Production-focused framework for search and QA.

| Aspect | Details |
|--------|---------|
| **Language** | Python |
| **Maturity** | Very mature (v2.0+) |
| **Ecosystem** | Search/NLP focused |
| **Learning curve** | Medium |
| **Best for** | Production search, document QA |

**Core Concepts:**

```python
from haystack import Pipeline
from haystack.components.retrievers import InMemoryBM25Retriever
from haystack.components.generators import OpenAIGenerator
from haystack.components.builders import PromptBuilder

# Build a RAG pipeline
pipe = Pipeline()
pipe.add_component("retriever", InMemoryBM25Retriever(document_store))
pipe.add_component("prompt", PromptBuilder(template="""
    Context: {{documents}}
    Question: {{query}}
    Answer:
"""))
pipe.add_component("llm", OpenAIGenerator())

pipe.connect("retriever", "prompt.documents")
pipe.connect("prompt", "llm")

# Run
result = pipe.run({"retriever": {"query": "What is RAG?"}})
```

**Strengths:**
- ‚úÖ Battle-tested in production
- ‚úÖ Excellent retrieval components
- ‚úÖ Clean, explicit pipelines
- ‚úÖ Good evaluation tools
- ‚úÖ Stable API

**Weaknesses:**
- ‚ùå Less flexible than LangChain
- ‚ùå Smaller ecosystem
- ‚ùå Fewer agent capabilities

**When to use:** Production search systems, document QA, when stability matters.

üîó [Haystack Documentation](https://docs.haystack.deepset.ai)

---

## RAG-Focused Frameworks

### LlamaIndex

The go-to framework for RAG applications.

| Aspect | Details |
|--------|---------|
| **Language** | Python, TypeScript |
| **Maturity** | Production-ready |
| **Ecosystem** | RAG-focused, growing |
| **Learning curve** | Low-Medium |
| **Best for** | Document QA, knowledge bases |

**Core Concepts:**

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# Load documents
documents = SimpleDirectoryReader("data/").load_data()

# Create index (embeddings + storage)
index = VectorStoreIndex.from_documents(documents)

# Query
query_engine = index.as_query_engine()
response = query_engine.query("What is the main topic?")
```

**Advanced RAG:**

```python
from llama_index.core import VectorStoreIndex
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.postprocessor import SimilarityPostprocessor

# Custom chunking
parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)

# Build with custom settings
index = VectorStoreIndex.from_documents(
    documents,
    transformations=[parser]
)

# Query with post-processing
query_engine = index.as_query_engine(
    similarity_top_k=10,
    node_postprocessors=[
        SimilarityPostprocessor(similarity_cutoff=0.7)
    ]
)
```

**Key Abstractions:**

| Abstraction | Purpose |
|-------------|---------|
| **Documents** | Raw content |
| **Nodes** | Chunks of documents |
| **Index** | Searchable structure |
| **Query Engine** | RAG interface |
| **Chat Engine** | Conversational RAG |
| **Agents** | Tool-using RAG |

**Strengths:**
- ‚úÖ Best-in-class RAG experience
- ‚úÖ Many index types (vector, keyword, graph)
- ‚úÖ Great data connectors
- ‚úÖ Easy to start, powerful to extend
- ‚úÖ Good documentation

**Weaknesses:**
- ‚ùå Less agent-focused than LangChain
- ‚ùå Can be opinionated
- ‚ùå Some advanced features need more setup

**When to use:** Any RAG application, document QA, knowledge bases.

üîó [LlamaIndex Documentation](https://docs.llamaindex.ai)

---

## Agent Frameworks

### AutoGen

Microsoft's multi-agent conversation framework.

```python
from autogen import AssistantAgent, UserProxyAgent

# Create agents
assistant = AssistantAgent(
    name="assistant",
    llm_config={"model": "gpt-4"}
)

user_proxy = UserProxyAgent(
    name="user",
    human_input_mode="NEVER",
    code_execution_config={"work_dir": "coding"}
)

# Start conversation
user_proxy.initiate_chat(
    assistant,
    message="Write a Python function to calculate fibonacci numbers."
)
```

**Best for:** Multi-agent systems, code generation workflows, research.

üîó [AutoGen Documentation](https://microsoft.github.io/autogen/)

---

### CrewAI

Role-based multi-agent orchestration.

```python
from crewai import Agent, Task, Crew

# Define agents with roles
researcher = Agent(
    role="Researcher",
    goal="Find accurate information",
    backstory="Expert researcher with attention to detail"
)

writer = Agent(
    role="Writer", 
    goal="Create engaging content",
    backstory="Skilled writer who makes complex topics simple"
)

# Define tasks
research_task = Task(
    description="Research the topic: {topic}",
    agent=researcher
)

write_task = Task(
    description="Write an article based on the research",
    agent=writer
)

# Create and run crew
crew = Crew(agents=[researcher, writer], tasks=[research_task, write_task])
result = crew.kickoff(inputs={"topic": "AI in healthcare"})
```

**Best for:** Role-based workflows, content generation, simulations.

üîó [CrewAI Documentation](https://docs.crewai.com)

---

## Lightweight / Specialized Tools

### Instructor

Structured outputs from LLMs using Pydantic.

```python
import instructor
from pydantic import BaseModel
from openai import OpenAI

# Patch the client
client = instructor.from_openai(OpenAI())

# Define your schema
class User(BaseModel):
    name: str
    age: int
    email: str

# Extract structured data
user = client.chat.completions.create(
    model="gpt-4o-mini",
    response_model=User,
    messages=[{"role": "user", "content": "John is 25, email: john@example.com"}]
)

print(user.name)  # "John"
print(user.age)   # 25
```

**Best for:** Structured extraction, type-safe LLM outputs, simple use cases.

üîó [Instructor Documentation](https://python.useinstructor.com)

---

### Marvin

AI functions for Python‚Äîmake any function AI-powered.

```python
import marvin

@marvin.fn
def sentiment(text: str) -> float:
    """Returns sentiment score from -1 (negative) to 1 (positive)"""

score = sentiment("I love this product!")  # Returns ~0.9

@marvin.fn  
def extract_entities(text: str) -> list[str]:
    """Extract named entities from text"""

entities = extract_entities("Apple CEO Tim Cook announced...")
# Returns ["Apple", "Tim Cook"]
```

**Best for:** Quick AI augmentation of existing code, classification, extraction.

üîó [Marvin Documentation](https://www.askmarvin.ai)

---

### Guidance

Constrained generation with templates.

```python
from guidance import models, gen, select

# Load model
lm = models.OpenAI("gpt-4o-mini")

# Constrained generation
lm + f'''
Question: What is 2+2?
Answer: {gen('answer', regex='[0-9]+')}

Is this correct? {select(['yes', 'no'], name='correct')}
'''
```

**Best for:** Controlled outputs, grammar-constrained generation.

üîó [Guidance Documentation](https://github.com/guidance-ai/guidance)

---

## Framework Comparison

### By Use Case

| Use Case | Best Framework | Why |
|----------|----------------|-----|
| Quick prototype | LangChain | Fastest to start |
| Production RAG | LlamaIndex | Purpose-built |
| .NET application | Semantic Kernel | Native support |
| Multi-agent | AutoGen / CrewAI | Specialized |
| Search system | Haystack | Battle-tested |
| Structured output | Instructor | Simple, typed |
| Complex workflows | LangGraph | Stateful graphs |

### By Team

| Team Type | Best Framework | Why |
|-----------|----------------|-----|
| Startup | LangChain + LlamaIndex | Speed, flexibility |
| Enterprise .NET | Semantic Kernel | Native integration |
| ML/Data team | Haystack | Production focus |
| Research | AutoGen | Experimentation |
| Small/Solo | Instructor/Marvin | Low overhead |

### Feature Matrix

| Feature | LangChain | LlamaIndex | Semantic Kernel | Haystack |
|---------|-----------|------------|-----------------|----------|
| RAG | ‚úÖ Good | ‚úÖ Best | ‚úÖ Good | ‚úÖ Great |
| Agents | ‚úÖ Best | ‚úÖ Good | ‚úÖ Good | ‚ö†Ô∏è Basic |
| Integrations | ‚úÖ Most | ‚úÖ Many | ‚ö†Ô∏è Growing | ‚úÖ Many |
| TypeScript | ‚úÖ Yes | ‚úÖ Yes | ‚ùå No | ‚ùå No |
| .NET | ‚ùå No | ‚ùå No | ‚úÖ Best | ‚ùå No |
| Observability | ‚úÖ LangSmith | ‚ö†Ô∏è Basic | ‚ö†Ô∏è Basic | ‚ö†Ô∏è Basic |
| Learning curve | Medium | Low | Medium | Medium |

---

## Integration Patterns

### LangChain + LlamaIndex

Use LlamaIndex for RAG, LangChain for orchestration:

```python
from llama_index.core import VectorStoreIndex
from langchain.tools import Tool
from langchain.agents import AgentExecutor

# LlamaIndex for retrieval
index = VectorStoreIndex.from_documents(docs)
query_engine = index.as_query_engine()

# Wrap as LangChain tool
rag_tool = Tool(
    name="knowledge_base",
    description="Search the knowledge base for information",
    func=lambda q: str(query_engine.query(q))
)

# Use in LangChain agent
agent = create_agent(llm, [rag_tool, other_tools])
```

### When NOT to Use a Framework

Sometimes direct SDK usage is better:

```python
# Simple use case - just use the SDK
from openai import OpenAI
client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

**Skip frameworks when:**
- Simple, single-model interactions
- You need maximum control
- Performance is critical
- Team prefers explicit code

---

## Getting Started Recommendations

### Beginner Path

1. **Start with LangChain** for broad exposure
2. **Add LlamaIndex** when building RAG
3. **Try Instructor** for structured outputs
4. **Graduate to LangGraph** for complex agents

### Enterprise Path

1. **Evaluate Semantic Kernel** if .NET shop
2. **Consider Haystack** for search focus
3. **Use Azure OpenAI** for compliance
4. **Add LangSmith** for observability

### Research Path

1. **Start with AutoGen** for multi-agent
2. **Experiment with CrewAI** for roles
3. **Use Guidance** for controlled generation
4. **Build custom** when frameworks limit you

---

## Further Reading

- [LangChain vs LlamaIndex Comparison](https://www.langchain.com)
- [Building LLM Applications](https://docs.llamaindex.ai)
- [Semantic Kernel Concepts](https://learn.microsoft.com/semantic-kernel/)
- [Haystack Tutorials](https://haystack.deepset.ai/tutorials)

---

*Part of the [AI Tools Landscape 2026](ai-tools-landscape-2026.md) series.*
